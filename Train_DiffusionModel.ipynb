{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLfV4UqLlRgM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from diffusers import AutoencoderKL\n",
    "from diffusionmodel import UNet, NoiseScheduler\n",
    "from losses import VGGLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IN1NqjeGV55e"
   },
   "outputs": [],
   "source": [
    "# weights initialization\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    if isinstance(m, nn.GroupNorm):\n",
    "        if hasattr(m, 'weight') and m.weight is not None:\n",
    "            torch.nn.init.constant_(m.weight, 1)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Use of the VAE to encode and decode images into latent space\n",
    "\n",
    "def encode_latents(images):\n",
    "    # images: [B,3,H,W], range [-1,1]\n",
    "    with torch.no_grad():\n",
    "        latents = vae.encode(images).latent_dist.sample()\n",
    "        latents = latents * 0.18215  # SD scaling factor\n",
    "    return latents\n",
    "\n",
    "def decode_latents(latents):\n",
    "    latents = latents / 0.18215\n",
    "    with torch.no_grad():\n",
    "        imgs = vae.decode(latents).sample\n",
    "    return imgs"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcECTan6DEKh"
   },
   "outputs": [],
   "source": [
    "# Load pretrained Stable Diffusion VAE\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    subfolder=\"vae\"\n",
    ").to(device)\n",
    "vae.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HloQbLIHZxNW"
   },
   "outputs": [],
   "source": [
    "original_images = torch.load('data/original_images_batch.pth')\n",
    "inputs_viton = torch.load('data/inputs_viton_batch.pth')\n",
    "mask_images = torch.load('data/mask_images_batch.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VoguK18NC67x"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lr = 0.00001\n",
    "mse_loss = nn.MSELoss().to(device)\n",
    "vgg_loss = VGGLoss().to(device)\n",
    "vgg_lambda = 0.001\n",
    "epochs = 150\n",
    "\n",
    "viton = UNet(9, 4).to(device)\n",
    "viton_opt = torch.optim.AdamW(viton.parameters(), lr=lr)\n",
    "\n",
    "viton = viton.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFk3gNosrQrw"
   },
   "outputs": [],
   "source": [
    "scheduler = NoiseScheduler(timesteps=1000, beta_schedule=\"cosine\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7We5JSdIS7v"
   },
   "outputs": [],
   "source": [
    "# Convert images into latent space\n",
    "\n",
    "z_original_images = []\n",
    "z_inputs_viton = []\n",
    "z_mask_images = []\n",
    "\n",
    "for original_image, input_viton, mask_image in zip(original_images_batch, inputs_viton_batch, mask_images_batch):\n",
    "    z_original_image = encode_latents(original_image)\n",
    "    z_input_viton = encode_latents(input_viton)\n",
    "    z_mask_image = F.interpolate(mask_image, size=z_original_image.shape[-2:], mode=\"nearest\")\n",
    "\n",
    "    z_original_images.append(z_original_image)\n",
    "    z_inputs_viton.append(z_input_viton)\n",
    "    z_mask_images.append(z_mask_image)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(101, epochs+1):\n",
    "\n",
    "  for original_image, z_original_image, input_viton, z_input_viton, mask_image, z_mask_image in zip(original_images, z_original_images, inputs_viton, z_inputs_viton, mask_images, z_mask_images):\n",
    "\n",
    "    x_0_original = original_image.to(device)\n",
    "\n",
    "    t = torch.randint(0, scheduler.timesteps, (1, )).to(device)\n",
    "\n",
    "    z_t_original, noise = scheduler.get_noisy_image(z_original_image, t)\n",
    "\n",
    "    x_0_agnostic = input_viton\n",
    "\n",
    "    z_t_agnostic, _ = scheduler.get_noisy_image(z_input_viton, t, noise)\n",
    "\n",
    "    input_original = torch.concat([z_t_original, z_input_viton, z_mask_image[:, 0:1, :, :]], dim=1)\n",
    "\n",
    "    unet_output_original = viton(input_original, torch.tensor([t]).to(device))\n",
    "\n",
    "    if epoch < 150:\n",
    "\n",
    "      loss = mse_loss(unet_output_original, noise)\n",
    "\n",
    "    else:\n",
    "\n",
    "      input_agnostic = torch.concat([z_t_agnostic, z_input_viton, z_mask_image[:, 0:1, :, :]], dim=1)\n",
    "\n",
    "      unet_output_agnostic = viton(input_agnostic, torch.tensor([t]).to(device))\n",
    "\n",
    "      z_denoise_agnostic = scheduler.denoise_image(z_t_agnostic, t, unet_output_agnostic)\n",
    "\n",
    "      denoise_agnostic = decode_latents(z_denoise_agnostic)\n",
    "\n",
    "      # Mask images needs to be rounded because the edges of the mask are not\n",
    "      # exactly one and we rescalar it as well to be [0,1]\n",
    "\n",
    "      mask_image = ((mask_image+1)/2).round().repeat(1, 3, 1, 1)\n",
    "\n",
    "      denoise_agnostic_masked_area = x_0_original.clone()\n",
    "\n",
    "      # For the VGG only the mask area will be part of the loss calculation.\n",
    "\n",
    "      denoise_agnostic_masked_area[mask_image == 1] = denoise_agnostic[mask_image == 1]\n",
    "\n",
    "      loss = mse_loss(unet_output_original, noise) + (vgg_lambda * vgg_loss(denoise_agnostic_masked_area, x_0_original))\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    viton_opt.zero_grad()\n",
    "    loss.backward()\n",
    "    viton_opt.step()\n",
    "\n",
    "    # Plot the output images to check the visual performance of the model.\n",
    "\n",
    "    if epoch == 150:\n",
    "\n",
    "      print(f\"Epoch {epoch}, Mean Loss: {np.mean(losses)} at timestep {t} and lr: {lr}\")\n",
    "\n",
    "      fig, axs = plt.subplots(1, 3, figsize=(10, 5))\n",
    "      axs[0].imshow(((original_image[0]).permute(1, 2, 0).detach().cpu().numpy()+1)/2)\n",
    "      axs[0].axis('off')\n",
    "\n",
    "      axs[1].imshow(((denoise_agnostic_masked_area[0]).permute(1, 2, 0).detach().cpu().numpy()+1)/2)\n",
    "      axs[1].axis('off')\n",
    "\n",
    "      axs[2].imshow(((denoise_agnostic[0]).permute(1, 2, 0).detach().cpu().numpy()+1)/2)\n",
    "      axs[2].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "  # if epoch % 10 == 0:\n",
    "\n",
    "  #   # Save the model checkpoint each 10 epochs\n",
    "  #   checkpoint = {\n",
    "  #       'epoch': epoch,\n",
    "  #       'model_state_dict': viton.state_dict(),\n",
    "  #       'optimizer_state_dict': viton_opt.state_dict(),\n",
    "  #       'loss': losses,\n",
    "  #   }\n",
    "\n",
    "  #   torch.save(checkpoint, f'/content/drive/MyDrive/AIClothes/DDPM/Models/Diffusion/latent512_2000_batch5_SelfAttention_cosinescheduler_resnet_adamW_lr{lr}_vgg{vgg_lambda}_epoch_{epoch}.pth')\n",
    "\n",
    "  print(f\"Epoch {epoch}, Mean Loss: {np.mean(losses)}\")"
   ],
   "metadata": {
    "id": "IbeN40bt-_oi"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
